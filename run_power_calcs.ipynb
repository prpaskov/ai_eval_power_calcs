{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Power calcs for y_i vs. y_j tests\n",
    "\n",
    "$$H_0: y_{LLM_i} = y_{LLM_j}$$\n",
    "$$H_1: y_{LLM_i} \\neq y_{LLM_j}$$\n",
    "\n",
    "This notebook takes in paired benchmark data and calculates stats needed for power calculations. It then runs grid search over parameters to report sample size requirements under a range of assumptions. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## 0. Set-up\n",
    "----\n",
    "\n",
    "$$n = (z_{\\alpha/2} + z_\\beta)^2 \\frac{(\\omega^2 + \\sigma_A^2/K_A + \\sigma_B^2/K_B)}{\\delta^2}$$\n",
    "\n",
    "$$\\textbf{For independent questions, either in single or multiple draws:}$$\n",
    "\n",
    "$$\\omega^2 = \\text{Var}_(x_A) + \\text{Var}_(x_B) - 2\\text{Cov}_(x_A, x_B)$$\n",
    "\n",
    "$$\\sigma_A^2 = E[\\sigma_{A,i}^2]$$\n",
    "\n",
    "$$\\sigma_B^2 = E[\\sigma_{B,i}^2]$$\n",
    "\n",
    "$$\\text{where sigma captures the standard deviation among the same question repeated and would thus be 0 with no repeated questions}$$\n",
    "\n",
    "\n",
    "$$\\textbf{For clustered questions, either in single or multiple draws:}$$\n",
    "\n",
    "$$\\omega_{clustered}^2 = \\text{Var}_{clustered}(x_A) + \\text{Var}_{clustered}(x_B) - 2\\text{Cov}_{clustered}(x_A, x_B)$$\n",
    "\n",
    "$$\\text{Var}_{clustered}(x_A) = \\frac{1}{n}\\sum_c \\sum_i \\sum_j (x_{A,i,c} - \\bar{s}_A)(x_{A,j,c} - \\bar{s}_A)$$\n",
    "\n",
    "$$\\text{Var}_{clustered}(x_B) = \\frac{1}{n}\\sum_c \\sum_i \\sum_j (x_{B,i,c} - \\bar{s}_B)(x_{B,j,c} - \\bar{s}_B)$$\n",
    "\n",
    "$$\\text{Cov}_{clustered}(x_A, x_B) = \\frac{1}{n}\\sum_c \\sum_i \\sum_j (x_{A,i,c} - \\bar{s}_A)(x_{B,j,c} - \\bar{s}_B)$$\n",
    "\n",
    "$$\\sigma_{A,clustered}^2 = \\frac{1}{n}\\sum_c \\sum_i \\sum_j \\epsilon_{A,i,c}\\epsilon_{A,j,c}$$\n",
    "\n",
    "$$\\sigma_{B,clustered}^2 = \\frac{1}{n}\\sum_c \\sum_i \\sum_j \\epsilon_{B,i,c}\\epsilon_{B,j,c}$$\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## 1. Define functions to calculate $\\omega^2$, $\\sigma^2$\n",
    "----\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, Any\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_omega_squared(df: pd.DataFrame, \n",
    "                           score1_col: str = 'score1', \n",
    "                           score2_col: str = 'score2',\n",
    "                           question_id_col: str = 'question_id',\n",
    "                           question_draw_col: str = 'question_draw', \n",
    "                           question_type_col: str = 'question_type') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate ω² adaptively based on dataset structure.\n",
    "    \n",
    "    Automatically detects:\n",
    "    - Whether questions are clustered (multiple question_types) based on whether there is >1 unique value of question_type\n",
    "    - Whether there are multiple draws per question (multiple question_draws), based on whether there is >1 unique value of question_draw\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Long format data with columns for question_id, question_draw, question_type, scores\n",
    "    score1_col : str\n",
    "        Column name for scorer 1's scores\n",
    "    score2_col : str  \n",
    "        Column name for scorer 2's scores\n",
    "    question_id_col : str\n",
    "        Column name for question identifiers\n",
    "    question_draw_col : str\n",
    "        Column name for question draw numbers\n",
    "    question_type_col : str\n",
    "        Column name for question types/clusters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results containing omega_squared and diagnostic information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input data\n",
    "    required_cols = [question_id_col, question_draw_col, question_type_col, score1_col, score2_col]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Detect dataset characteristics\n",
    "    n_question_types = df[question_type_col].nunique()\n",
    "    n_draws_per_question = df.groupby(question_id_col)[question_draw_col].nunique().max()\n",
    "    n_total_questions = df[question_id_col].nunique()\n",
    "    \n",
    "    is_clustered = n_question_types > 1\n",
    "    has_multiple_draws = n_draws_per_question > 1\n",
    "    \n",
    "    print(f\"Dataset characteristics:\")\n",
    "    print(f\"  - Total unique questions: {n_total_questions}\")\n",
    "    print(f\"  - Question types/clusters: {n_question_types}\")\n",
    "    print(f\"  - Max draws per question: {n_draws_per_question}\")\n",
    "    print(f\"  - Clustered analysis: {is_clustered}\")\n",
    "    print(f\"  - Multiple draws: {has_multiple_draws}\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate conditional means for each question (averaging across draws if multiple)\n",
    "    question_means = df.groupby([question_id_col, question_type_col]).agg({\n",
    "        score1_col: 'mean',\n",
    "        score2_col: 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    question_means.columns = [question_id_col, question_type_col, 'mean_A', 'mean_B']\n",
    "    \n",
    "    # Calculate overall means\n",
    "    overall_mean_A = question_means['mean_A'].mean()\n",
    "    overall_mean_B = question_means['mean_B'].mean()\n",
    "    \n",
    "    if is_clustered:\n",
    "        # Use clustered variance/covariance calculations\n",
    "        omega_squared = calculate_clustered_omega_squared(\n",
    "            question_means, question_type_col, overall_mean_A, overall_mean_B\n",
    "        )\n",
    "        method = \"clustered\"\n",
    "    else:\n",
    "        # Use simple variance/covariance calculations\n",
    "        var_A = np.var(question_means['mean_A'], ddof=1)\n",
    "        var_B = np.var(question_means['mean_B'], ddof=1)\n",
    "        cov_AB = np.cov(question_means['mean_A'], question_means['mean_B'], ddof=1)[0, 1]\n",
    "        omega_squared = var_A + var_B - 2 * cov_AB\n",
    "        method = \"simple\"\n",
    "    \n",
    "    # Calculate sigma_squared terms if multiple draws available\n",
    "    if has_multiple_draws:\n",
    "        sigma_A_squared, sigma_B_squared = calculate_conditional_variances(\n",
    "            df, question_id_col, question_draw_col, question_type_col, \n",
    "            score1_col, score2_col, is_clustered\n",
    "        )\n",
    "    else:\n",
    "        sigma_A_squared = 0.0\n",
    "        sigma_B_squared = 0.0\n",
    "        print(\"Note: Only single draw per question, so σ²_A = σ²_B = 0\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'omega_squared': omega_squared,\n",
    "        'sigma_A_squared': sigma_A_squared,\n",
    "        'sigma_B_squared': sigma_B_squared,\n",
    "        'method': method,\n",
    "        'is_clustered': is_clustered,\n",
    "        'has_multiple_draws': has_multiple_draws,\n",
    "        'n_questions': n_total_questions,\n",
    "        'n_question_types': n_question_types,\n",
    "        'n_draws_per_question': n_draws_per_question,\n",
    "        'var_A': np.var(question_means['mean_A'], ddof=1),\n",
    "        'var_B': np.var(question_means['mean_B'], ddof=1),\n",
    "        'cov_AB': np.cov(question_means['mean_A'], question_means['mean_B'], ddof=1)[0, 1]\n",
    "    }\n",
    "    \n",
    "    print(f\"Results using {method} method:\")\n",
    "    print(f\"  ω² = {omega_squared:.6f}\")\n",
    "    print(f\"  σ²_A = {sigma_A_squared:.6f}\")\n",
    "    print(f\"  σ²_B = {sigma_B_squared:.6f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_clustered_omega_squared(question_means: pd.DataFrame, \n",
    "                                    question_type_col: str,\n",
    "                                    overall_mean_A: float,\n",
    "                                    overall_mean_B: float) -> float:\n",
    "    \"\"\"Calculate ω² using clustered variance/covariance formulas.\"\"\"\n",
    "    \n",
    "    n = len(question_means)\n",
    "    \n",
    "    # Calculate clustered variances and covariance\n",
    "    var_A_clustered = 0\n",
    "    var_B_clustered = 0\n",
    "    cov_AB_clustered = 0\n",
    "    \n",
    "    for question_type in question_means[question_type_col].unique():\n",
    "        cluster_data = question_means[question_means[question_type_col] == question_type]\n",
    "        \n",
    "        for _, row_i in cluster_data.iterrows():\n",
    "            for _, row_j in cluster_data.iterrows():\n",
    "                # Variance A component\n",
    "                var_A_clustered += (row_i['mean_A'] - overall_mean_A) * (row_j['mean_A'] - overall_mean_A)\n",
    "                \n",
    "                # Variance B component\n",
    "                var_B_clustered += (row_i['mean_B'] - overall_mean_B) * (row_j['mean_B'] - overall_mean_B)\n",
    "                \n",
    "                # Covariance component\n",
    "                cov_AB_clustered += (row_i['mean_A'] - overall_mean_A) * (row_j['mean_B'] - overall_mean_B)\n",
    "    \n",
    "    var_A_clustered /= n\n",
    "    var_B_clustered /= n\n",
    "    cov_AB_clustered /= n\n",
    "    \n",
    "    omega_squared = var_A_clustered + var_B_clustered - 2 * cov_AB_clustered\n",
    "    \n",
    "    return omega_squared\n",
    "\n",
    "def calculate_conditional_variances(df: pd.DataFrame,\n",
    "                                  question_id_col: str,\n",
    "                                  question_draw_col: str, \n",
    "                                  question_type_col: str,\n",
    "                                  score1_col: str,\n",
    "                                  score2_col: str,\n",
    "                                  is_clustered: bool) -> Tuple[float, float]:\n",
    "    \"\"\"Calculate σ²_A and σ²_B (conditional variances).\"\"\"\n",
    "    \n",
    "    n_total = df[question_id_col].nunique()\n",
    "    \n",
    "    if is_clustered:\n",
    "        return calculate_clustered_conditional_variances(\n",
    "            df, question_id_col, question_draw_col, question_type_col,\n",
    "            score1_col, score2_col, n_total\n",
    "        )\n",
    "    else:\n",
    "        return calculate_simple_conditional_variances(\n",
    "            df, question_id_col, score1_col, score2_col\n",
    "        )\n",
    "\n",
    "def calculate_simple_conditional_variances(df: pd.DataFrame,\n",
    "                                         question_id_col: str,\n",
    "                                         score1_col: str,\n",
    "                                         score2_col: str) -> Tuple[float, float]:\n",
    "    \"\"\"Calculate simple (non-clustered) conditional variances.\"\"\"\n",
    "    \n",
    "    # Calculate conditional variance for each question\n",
    "    question_variances = df.groupby(question_id_col).agg({\n",
    "        score1_col: lambda x: np.var(x, ddof=1) if len(x) > 1 else 0,\n",
    "        score2_col: lambda x: np.var(x, ddof=1) if len(x) > 1 else 0\n",
    "    })\n",
    "    \n",
    "    # Expected conditional variances\n",
    "    sigma_A_squared = question_variances[score1_col].mean()\n",
    "    sigma_B_squared = question_variances[score2_col].mean()\n",
    "    \n",
    "    return sigma_A_squared, sigma_B_squared\n",
    "\n",
    "def calculate_clustered_conditional_variances(df: pd.DataFrame,\n",
    "                                            question_id_col: str,\n",
    "                                            question_draw_col: str,\n",
    "                                            question_type_col: str,\n",
    "                                            score1_col: str,\n",
    "                                            score2_col: str,\n",
    "                                            n_total: int) -> Tuple[float, float]:\n",
    "    \"\"\"Calculate clustered conditional variances using the paper's formula.\"\"\"\n",
    "    \n",
    "    # Calculate conditional means for each question\n",
    "    question_means = df.groupby([question_id_col, question_type_col]).agg({\n",
    "        score1_col: 'mean',\n",
    "        score2_col: 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Merge back to get conditional means for each observation\n",
    "    df_with_means = df.merge(question_means, on=[question_id_col, question_type_col], \n",
    "                            suffixes=('', '_mean'))\n",
    "    \n",
    "    K = df.groupby(question_id_col)[question_draw_col].nunique().max()\n",
    "    \n",
    "    # Calculate clustered conditional variances\n",
    "    sigma_A_clustered = 0\n",
    "    sigma_B_clustered = 0\n",
    "    \n",
    "    for question_type in df_with_means[question_type_col].unique():\n",
    "        cluster_data = df_with_means[df_with_means[question_type_col] == question_type]\n",
    "        \n",
    "        for _, row_i in cluster_data.iterrows():\n",
    "            for _, row_j in cluster_data.iterrows():\n",
    "                # Calculate epsilon values (residuals from conditional means)\n",
    "                epsilon_A_i = row_i[score1_col] - row_i[f'{score1_col}_mean']\n",
    "                epsilon_A_j = row_j[score1_col] - row_j[f'{score1_col}_mean']\n",
    "                epsilon_B_i = row_i[score2_col] - row_i[f'{score2_col}_mean']\n",
    "                epsilon_B_j = row_j[score2_col] - row_j[f'{score2_col}_mean']\n",
    "                \n",
    "                sigma_A_clustered += epsilon_A_i * epsilon_A_j\n",
    "                sigma_B_clustered += epsilon_B_i * epsilon_B_j\n",
    "    \n",
    "    sigma_A_clustered /= (n_total * (K - 1))\n",
    "    sigma_B_clustered /= (n_total * (K - 1))\n",
    "    \n",
    "    return sigma_A_clustered, sigma_B_clustered\n",
    "\n",
    "def create_sample_data() -> pd.DataFrame:\n",
    "    \"\"\"Create sample data to demonstrate the function.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = []\n",
    "    question_id = 0\n",
    "    \n",
    "    # Create data with 2 question types, 10 questions each, 3 draws per question\n",
    "    for question_type in ['reading_comp', 'math']:\n",
    "        for q in range(10):\n",
    "            question_id += 1\n",
    "            base_difficulty_A = np.random.uniform(0.4, 0.8)\n",
    "            base_difficulty_B = base_difficulty_A + np.random.normal(0, 0.1)\n",
    "            \n",
    "            for draw in range(1, 4):  # 3 draws per question\n",
    "                score_A = np.random.binomial(1, base_difficulty_A)\n",
    "                score_B = np.random.binomial(1, np.clip(base_difficulty_B, 0, 1))\n",
    "                \n",
    "                data.append({\n",
    "                    'question_id': f'q_{question_id}',\n",
    "                    'question_draw': draw,\n",
    "                    'question_type': question_type,\n",
    "                    'scorer1_id': 'model_A',\n",
    "                    'scorer2_id': 'model_B', \n",
    "                    'score1': score_A,\n",
    "                    'score2': score_B\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## 2. Define functions for power calcs with grid search\n",
    "----\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(omega_squared: float, \n",
    "                        sigma_A_squared: float, \n",
    "                          sigma_B_squared: float, \n",
    "                          z_alpha_2: float, \n",
    "                          z_beta: float, \n",
    "                          K_A: int, \n",
    "                          K_B: int, \n",
    "                          delta: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate required sample size\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    omega_squared : float\n",
    "        Variance of score differences (ω²)\n",
    "    sigma_A_squared : float  \n",
    "        Expected conditional variance for model A (σ²_A)\n",
    "    sigma_B_squared : float\n",
    "        Expected conditional variance for model B (σ²_B)\n",
    "    z_alpha_2 : float\n",
    "        Critical value for α/2 (two-tailed test)\n",
    "    z_beta : float\n",
    "        Critical value for β (Type II error)\n",
    "    K_A : int\n",
    "        Number of samples per question for model A\n",
    "    K_B : int\n",
    "        Number of samples per question for model B\n",
    "    delta : float\n",
    "        Minimum detectable effect size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    n : float\n",
    "        Required number of questions\n",
    "    \"\"\"\n",
    "    numerator = (z_alpha_2 + z_beta)**2 * (omega_squared + sigma_A_squared/K_A + sigma_B_squared/K_B)\n",
    "    denominator = delta**2\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def calculate_minimum_detectable_effect(omega_squared, sigma_A_squared, sigma_B_squared,\n",
    "                                      z_alpha_2, z_beta, K_A, K_B, n):\n",
    "    \"\"\"\n",
    "    Calculate minimum detectable effect for given sample size.\n",
    "    Inverted version of the sample size formula.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    delta : float\n",
    "        Minimum detectable effect size\n",
    "    \"\"\"\n",
    "    variance_term = omega_squared + sigma_A_squared/K_A + sigma_B_squared/K_B\n",
    "    delta = (z_alpha_2 + z_beta) * np.sqrt(variance_term / n)\n",
    "    \n",
    "    return delta\n",
    "\n",
    "def create_power_analysis_grid():\n",
    "    \"\"\"\n",
    "    Create a comprehensive grid of parameters for power analysis.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Parameter grid with all combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standard significance levels and their z-values (two sided)\n",
    "    significance_levels = {\n",
    "        'alpha_0.01': {'alpha': 0.01, 'z_alpha_2': stats.norm.ppf(1 - 0.01/2)},  # z ≈ 2.576\n",
    "        'alpha_0.05': {'alpha': 0.05, 'z_alpha_2': stats.norm.ppf(1 - 0.05/2)},  # z ≈ 1.960  \n",
    "        'alpha_0.10': {'alpha': 0.10, 'z_alpha_2': stats.norm.ppf(1 - 0.10/2)},  # z ≈ 1.645\n",
    "    }\n",
    "    \n",
    "    # Standard power levels and their z-values\n",
    "    power_levels = {\n",
    "        'power_0.80': {'beta': 0.20, 'power': 0.80, 'z_beta': stats.norm.ppf(1 - 0.20)},  # z ≈ 0.842\n",
    "        'power_0.85': {'beta': 0.15, 'power': 0.85, 'z_beta': stats.norm.ppf(1 - 0.15)},  # z ≈ 1.036\n",
    "        'power_0.90': {'beta': 0.10, 'power': 0.90, 'z_beta': stats.norm.ppf(1 - 0.10)},  # z ≈ 1.282\n",
    "        'power_0.95': {'beta': 0.05, 'power': 0.95, 'z_beta': stats.norm.ppf(1 - 0.05)},  # z ≈ 1.645\n",
    "    }\n",
    "    \n",
    "    # Sampling strategies (K values)\n",
    "    K_values = [1, 2, 3, 5, 10]  # Number of samples per question\n",
    "    \n",
    "    # Create the parameter grid\n",
    "    grid = {\n",
    "        'significance_levels': significance_levels,\n",
    "        'power_levels': power_levels, \n",
    "        'K_values': K_values\n",
    "    }\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def create_delta_grid(var_x_A, var_x_B, n_points=10):\n",
    "    \"\"\"\n",
    "    Create a grid of delta values based on the standard deviations of x_A and x_B.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    var_x_A : float\n",
    "        Variance of conditional means for model A\n",
    "    var_x_B : float  \n",
    "        Variance of conditional means for model B\n",
    "    n_points : int\n",
    "        Number of delta values to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Delta values ranging from small to large effects\n",
    "    \"\"\"\n",
    "    # Calculate standard deviations\n",
    "    sd_x_A = np.sqrt(var_x_A)\n",
    "    sd_x_B = np.sqrt(var_x_B)\n",
    "    avg_sd = (sd_x_A + sd_x_B) / 2\n",
    "    \n",
    "    # Create delta values as fractions of the average standard deviation\n",
    "    # Small effect: 0.1 * SD, Medium: 0.5 * SD, Large: 1.0 * SD\n",
    "    effect_sizes = np.linspace(0.05, 1.5, n_points)  # From 5% to 150% of SD\n",
    "    delta_values = effect_sizes * avg_sd\n",
    "    \n",
    "    return delta_values.tolist()\n",
    "\n",
    "def run_power_analysis_grid(omega_squared, sigma_A_squared, sigma_B_squared,\n",
    "                           var_x_A, var_x_B, max_n=10000):\n",
    "    \"\"\"\n",
    "    Run comprehensive power analysis across parameter grid.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    omega_squared : float\n",
    "        Estimated ω² from your data\n",
    "    sigma_A_squared : float\n",
    "        Estimated σ²_A from your data  \n",
    "    sigma_B_squared : float\n",
    "        Estimated σ²_B from your data\n",
    "    var_x_A : float\n",
    "        Variance of conditional means for model A\n",
    "    var_x_B : float\n",
    "        Variance of conditional means for model B\n",
    "    max_n : int\n",
    "        Maximum sample size to consider practical\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Results of grid search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create parameter grids\n",
    "    param_grid = create_power_analysis_grid()\n",
    "    delta_values = create_delta_grid(var_x_A, var_x_B)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Grid search over all combinations\n",
    "    for sig_name, sig_params in param_grid['significance_levels'].items():\n",
    "        for pow_name, pow_params in param_grid['power_levels'].items():\n",
    "            for K in param_grid['K_values']:\n",
    "                for delta in delta_values:\n",
    "                    \n",
    "                    # Calculate required sample size\n",
    "                    n_required = calculate_sample_size(\n",
    "                        omega_squared, sigma_A_squared, sigma_B_squared,\n",
    "                        sig_params['z_alpha_2'], pow_params['z_beta'],\n",
    "                        K, K, delta  # Assuming K_A = K_B = K\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate effect size relative to average SD\n",
    "                    avg_sd = (np.sqrt(var_x_A) + np.sqrt(var_x_B)) / 2\n",
    "                    relative_effect = delta / avg_sd if avg_sd > 0 else np.nan\n",
    "                    \n",
    "                    # Store results\n",
    "                    results.append({\n",
    "                        'alpha': sig_params['alpha'],\n",
    "                        'power': pow_params['power'],\n",
    "                        'beta': pow_params['beta'],\n",
    "                        'K': K,\n",
    "                        'delta': delta,\n",
    "                        'delta_relative_sd': relative_effect,\n",
    "                        'n_required': n_required,\n",
    "                        'feasible': n_required <= max_n,\n",
    "                        'z_alpha_2': sig_params['z_alpha_2'],\n",
    "                        'z_beta': pow_params['z_beta']\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_power_results(results_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze and summarize power analysis results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pd.DataFrame\n",
    "        Results from run_power_analysis_grid()\n",
    "    top_n : int\n",
    "        Number of top recommendations to show\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Analysis summary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to feasible options\n",
    "    feasible = results_df[results_df['feasible']].copy()\n",
    "    \n",
    "    if len(feasible) == 0:\n",
    "        print(\"⚠️  No feasible options found! Consider:\")\n",
    "        print(\"   - Increasing max_n\")\n",
    "        print(\"   - Accepting larger effect sizes\")\n",
    "        print(\"   - Reducing power requirements\")\n",
    "        return None\n",
    "    \n",
    "    # Find optimal configurations\n",
    "    print(\"🎯 POWER ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Most efficient (smallest n) for different effect sizes\n",
    "    print(\"\\n📊 Most Efficient Configurations (Smallest Sample Size):\")\n",
    "    efficient = feasible.nsmallest(top_n, 'n_required')[\n",
    "        ['alpha', 'power', 'K', 'delta_relative_sd', 'n_required']\n",
    "    ].round(3)\n",
    "    print(efficient.to_string(index=False))\n",
    "    \n",
    "    # Different effect size categories\n",
    "    small_effect = feasible[feasible['delta_relative_sd'] <= 0.3]\n",
    "    medium_effect = feasible[(feasible['delta_relative_sd'] > 0.3) & (feasible['delta_relative_sd'] <= 0.7)]\n",
    "    large_effect = feasible[feasible['delta_relative_sd'] > 0.7]\n",
    "    \n",
    "    print(f\"\\n📈 Sample Size Ranges:\")\n",
    "    print(f\"Small effects (≤0.3 SD): {small_effect['n_required'].min():.0f} - {small_effect['n_required'].max():.0f} questions\")\n",
    "    print(f\"Medium effects (0.3-0.7 SD): {medium_effect['n_required'].min():.0f} - {medium_effect['n_required'].max():.0f} questions\")  \n",
    "    print(f\"Large effects (>0.7 SD): {large_effect['n_required'].min():.0f} - {large_effect['n_required'].max():.0f} questions\")\n",
    "    \n",
    "    # Impact of K (sampling strategy)\n",
    "    print(f\"\\n🔄 Impact of Multiple Sampling (K):\")\n",
    "    k_impact = feasible.groupby('K')['n_required'].agg(['mean', 'min']).round(0)\n",
    "    print(k_impact)\n",
    "    \n",
    "    return {\n",
    "        'feasible_options': len(feasible),\n",
    "        'min_n': feasible['n_required'].min(),\n",
    "        'max_n': feasible['n_required'].max(),\n",
    "        'recommended': efficient.iloc[0].to_dict() if len(efficient) > 0 else None\n",
    "    }\n",
    "\n",
    "def plot_power_analysis(results_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create visualizations of power analysis results.\n",
    "    \"\"\"\n",
    "    feasible = results_df[results_df['feasible']].copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Power Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Sample size vs effect size\n",
    "    ax1 = axes[0, 0]\n",
    "    for alpha in feasible['alpha'].unique():\n",
    "        for power in feasible['power'].unique():\n",
    "            subset = feasible[(feasible['alpha'] == alpha) & \n",
    "                            (feasible['power'] == power) & \n",
    "                            (feasible['K'] == 1)]\n",
    "            if len(subset) > 0:\n",
    "                ax1.plot(subset['delta_relative_sd'], subset['n_required'], \n",
    "                        'o-', label=f'α={alpha}, power={power}', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Effect Size (relative to SD)')\n",
    "    ax1.set_ylabel('Required Sample Size')\n",
    "    ax1.set_title('Sample Size vs Effect Size (K=1)')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Impact of K\n",
    "    ax2 = axes[0, 1]\n",
    "    for K in sorted(feasible['K'].unique()):\n",
    "        subset = feasible[(feasible['K'] == K) & \n",
    "                         (feasible['alpha'] == 0.05) & \n",
    "                         (feasible['power'] == 0.8)]\n",
    "        if len(subset) > 0:\n",
    "            ax2.plot(subset['delta_relative_sd'], subset['n_required'], \n",
    "                    'o-', label=f'K={K}', alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Effect Size (relative to SD)')\n",
    "    ax2.set_ylabel('Required Sample Size')\n",
    "    ax2.set_title('Impact of Multiple Sampling (α=0.05, power=0.8)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Heatmap of feasible configurations\n",
    "    ax3 = axes[1, 0]\n",
    "    pivot = feasible.pivot_table(values='n_required', \n",
    "                               index=['alpha', 'power'], \n",
    "                               columns='K', \n",
    "                               aggfunc='mean')\n",
    "    sns.heatmap(pivot, annot=True, fmt='.0f', cmap='viridis_r', ax=ax3)\n",
    "    ax3.set_title('Average Sample Size by Configuration')\n",
    "    \n",
    "    # 4. Effect size recommendations\n",
    "    ax4 = axes[1, 1]\n",
    "    bins = [0, 0.3, 0.7, 1.0, 2.0]\n",
    "    labels = ['Small\\n(≤0.3 SD)', 'Medium\\n(0.3-0.7 SD)', 'Large\\n(0.7-1.0 SD)', 'Very Large\\n(>1.0 SD)']\n",
    "    feasible['effect_category'] = pd.cut(feasible['delta_relative_sd'], bins=bins, labels=labels, include_lowest=True)\n",
    "    \n",
    "    effect_summary = feasible.groupby('effect_category')['n_required'].agg(['mean', 'min', 'max'])\n",
    "    effect_summary.plot(kind='bar', ax=ax4)\n",
    "    ax4.set_title('Sample Size by Effect Category')\n",
    "    ax4.set_ylabel('Required Sample Size')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## 3. Generate sample data\n",
    "----\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data(n_question_types=2, n_questions_per_type=10, n_draws=3, \n",
    "                      add_correlation=True, seed=42):\n",
    "    \"\"\"\n",
    "    Create sample data in long format for LLM evaluation analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_question_types : int\n",
    "        Number of question types/clusters (default: 2)\n",
    "    n_questions_per_type : int\n",
    "        Number of questions per type (default: 10)\n",
    "    n_draws : int\n",
    "        Number of draws/samples per question (default: 3)\n",
    "    add_correlation : bool\n",
    "        Whether to add correlation between scorers (default: True)\n",
    "    seed : int\n",
    "        Random seed for reproducibility (default: 42)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Long format data with columns:\n",
    "        - question_id: Unique identifier for each question\n",
    "        - question_draw: Draw number (1, 2, 3, ...)\n",
    "        - question_type: Type/cluster of question\n",
    "        - scorer1_id: Always 'model_A'\n",
    "        - scorer2_id: Always 'model_B'\n",
    "        - score1: Score from model A\n",
    "        - score2: Score from model B\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data = []\n",
    "    question_id = 0\n",
    "    \n",
    "    # Question types (clusters)\n",
    "    question_types = [f'type_{i}' for i in range(n_question_types)]\n",
    "    \n",
    "    for question_type in question_types:\n",
    "        # Each question type has different base difficulty\n",
    "        type_difficulty_A = np.random.uniform(0.3, 0.8)\n",
    "        type_difficulty_B = np.random.uniform(0.3, 0.8)\n",
    "        \n",
    "        for q in range(n_questions_per_type):\n",
    "            question_id += 1\n",
    "            \n",
    "            # Individual question difficulty (varies around type difficulty)\n",
    "            base_difficulty_A = type_difficulty_A + np.random.normal(0, 0.15)\n",
    "            base_difficulty_A = np.clip(base_difficulty_A, 0.05, 0.95)\n",
    "            \n",
    "            if add_correlation:\n",
    "                # Model B performance correlates with Model A (similar questions are hard/easy for both)\n",
    "                correlation_strength = 0.6\n",
    "                base_difficulty_B = (correlation_strength * base_difficulty_A + \n",
    "                                   (1 - correlation_strength) * type_difficulty_B + \n",
    "                                   np.random.normal(0, 0.1))\n",
    "            else:\n",
    "                base_difficulty_B = type_difficulty_B + np.random.normal(0, 0.15)\n",
    "            \n",
    "            base_difficulty_B = np.clip(base_difficulty_B, 0.05, 0.95)\n",
    "            \n",
    "            # Generate multiple draws for this question\n",
    "            for draw in range(1, n_draws + 1):\n",
    "                # Add some randomness to each draw (conditional variance)\n",
    "                prob_A = np.clip(base_difficulty_A + np.random.normal(0, 0.05), 0, 1)\n",
    "                prob_B = np.clip(base_difficulty_B + np.random.normal(0, 0.05), 0, 1)\n",
    "                \n",
    "                score_A = np.random.binomial(1, prob_A)\n",
    "                score_B = np.random.binomial(1, prob_B)\n",
    "                \n",
    "                data.append({\n",
    "                    'question_id': f'q_{question_id:03d}',\n",
    "                    'question_draw': draw,\n",
    "                    'question_type': question_type,\n",
    "                    'scorer1_id': 'model_A',\n",
    "                    'scorer2_id': 'model_B',\n",
    "                    'score1': score_A,\n",
    "                    'score2': score_B\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## 4. Run\n",
    "----\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = create_sample_data()\n",
    "print(\"Sample data shape:\", sample_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(sample_df.head())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Calculate state variables squared\n",
    "results = calculate_omega_squared(sample_df)\n",
    "print(f\"Use ω² = {results['omega_squared']:.6f}\")\n",
    "print(f\"Use σ²_A = {results['sigma_A_squared']:.6f}\")  \n",
    "print(f\"Use σ²_B = {results['sigma_B_squared']:.6f}\")\n",
    "\n",
    "# Run power analysis grid search\n",
    "results = run_power_analysis_grid(\n",
    "    omega_squared=results['omega_squared'], \n",
    "    sigma_A_squared=results['sigma_A_squared'], \n",
    "    sigma_B_squared=results['sigma_B_squared'],\n",
    "    var_x_A=results['var_x_A'], \n",
    "    var_x_B=results['var_x_B'],\n",
    "    max_n=5000\n",
    "    )\n",
    "\n",
    "# Analyze results\n",
    "summary = analyze_power_results(results)\n",
    "plot_power_analysis(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
